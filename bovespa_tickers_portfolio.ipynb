{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bovespa_tickers_portfolio.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMFTjYBLvJjVnRwJnDR5eOd"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "674YARVbw_Lr"
      },
      "source": [
        "## Carrengando os dados (Loading the data)\n",
        "\n",
        "Os dados serão armazenados em dois dataframes:\n",
        "\n",
        "* **df_stocks**: all the stocks\n",
        "* **df_bench**: only the benchmarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8WEdjksxbqe"
      },
      "source": [
        "1. Importando Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI2Wre-Fw2mO"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import dateutil\n",
        "import glob\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZweHXqew_jZ"
      },
      "source": [
        "2. Listando dataframes previamente armazenados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8AHAE44w_5x"
      },
      "source": [
        "# listing pandas dataframes previously saved\n",
        "lst_df_path = glob.glob(os.path.join('/content/raw', 'df_*.pickle'))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYQLIq5gxAjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d69a94aa-fc12-4137-820b-000007a6b865"
      },
      "source": [
        "# checking the path and file names\n",
        "#lst_df_path[:3]\n",
        "lst_df_path[:]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vExDHzeyBEm"
      },
      "source": [
        "# remove the ticker that will be used for Benchmarks later\n",
        "lst_df_path.remove('/content/raw/df_BVSP.pickle')\n",
        "lst_df_path.remove('/content/raw/df_USDBRL.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgxcdpF9ySIf"
      },
      "source": [
        "# creating a separed list for the Benchmarks\n",
        "lst_df_path_bench = ['/content/raw/df_BVSP.pickle', '/content/raw/df_USDBRL.pickle']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBzcS1IzyUlR"
      },
      "source": [
        "# concatenating all stocks into one dataframe\n",
        "lst_df_stocks = []\n",
        "\n",
        "for fname in lst_df_path:\n",
        "    df = pd.read_pickle(fname)\n",
        "    # keeping only Adj Close\n",
        "    #df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Open', 'Adj High', 'Adj Low'], inplace=True)\n",
        "    df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
        "    ticker = fname.split('/content/raw/')[1].split('df_')[1].split('.')[0] \n",
        "    df.columns = [ticker]\n",
        "    lst_df_stocks.append(df)\n",
        "    \n",
        "df_stocks = pd.concat(lst_df_stocks, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byq4JsIYyWzt"
      },
      "source": [
        "df_stocks = pd.concat(lst_df_stocks, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie2cw3C7yZGn"
      },
      "source": [
        "# checking column names\n",
        "df_stocks.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD-6M7UiydhW"
      },
      "source": [
        "# concatenating the benchmarks into one dataframe\n",
        "lst_df_bench = []\n",
        "\n",
        "for fname in lst_df_path_bench:\n",
        "    df = pd.read_pickle(fname)\n",
        "    # keeping only Adj Close\n",
        "    #df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Open', 'Adj High', 'Adj Low'], inplace=True)\n",
        "    df.drop(columns=['Open', 'High', 'Low', 'Close', 'Volume'], inplace=True)\n",
        "    ticker = fname.split('/content/raw/')[1].split('df_')[1].split('.')[0] \n",
        "    df.columns = [ticker]\n",
        "    lst_df_bench.append(df)\n",
        "    \n",
        "df_bench = pd.concat(lst_df_bench, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHBGJ297ygV9"
      },
      "source": [
        "df_bench.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiUquaeKyhV_"
      },
      "source": [
        "df_bench.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw0iLfokxBA_"
      },
      "source": [
        "## Portfólio Otimizado Mensal\n",
        "\n",
        "O objetivo é compor uma carteira com bom desempenho utilizando apenas uma pequena quantidade de ações da lista.\n",
        "\n",
        "A cada mês será elaborada uma nova carteira com base no Índice Sharpe dos meses anteriores, e seu desempenho será comparado com três benchmarks:\n",
        "\n",
        "* iBovespa: Índice oficial da Bovespa (composto por +60 ações)\n",
        "\n",
        "* Média BVSP: média simples de todas as ações disponíveis da iBovespa\n",
        "\n",
        "* Dolar: O valor atual dos dólares americanos em reais\n",
        "\n",
        "**Restrições adicionais ao portfólio:**\n",
        "\n",
        "O peso máximo de uma ação é de 25%\n",
        "O peso mínimo de uma ação é 2%\n",
        "\n",
        "**Resultados esperados:**\n",
        "\n",
        "* desempenho aprimorado no longo prazo\n",
        "* maior volatilidade que o iBovespa, devido ao pequeno número de ações que compõem a carteira\n",
        "\n",
        "**Configurando a otimização**\n",
        "\n",
        "*Baseado no curso Udemy de Jose Portilla em [Python para algoritmo financeiro e comercial.](https://www.udemy.com/python-for-finance-and-trading-algorithms/learn/v4/)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRUArlKVxBUr"
      },
      "source": [
        "from scipy.optimize import minimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPzVVzYAobk9"
      },
      "source": [
        "# utility function to obtain the expected Return, expected Volatity, and Sharpe Ration from the log returns, given the weights\n",
        "def get_ret_vol_sr(weights):\n",
        "    global log_ret\n",
        "    weights = np.array(weights)    \n",
        "    ret = np.sum( log_ret.mean() * weights * 252)\n",
        "    vol = np.sqrt( np.dot(weights.T, np.dot(log_ret.cov()*252, weights)))\n",
        "    sr = ret/vol\n",
        "    return np.array([ret, vol, sr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5lY2a6Cocnh"
      },
      "source": [
        "# the actual function to be minimized\n",
        "def neg_sharpe(weights):\n",
        "    return -1.*get_ret_vol_sr(weights)[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByzTjpbEogum"
      },
      "source": [
        "# contraint function\n",
        "def check_sum(weights):\n",
        "    return np.sum(weights) - 1."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOVOdhSLoipS"
      },
      "source": [
        "# contraint function\n",
        "def check_max_weight(weights):\n",
        "    global max_weight\n",
        "    return np.minimum(weights.max(), max_weight) - weights.max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VVWYsXZokVh"
      },
      "source": [
        "# contraint function\n",
        "def check_weights(weights):\n",
        "    global max_weight\n",
        "    w1 = np.sum(weights) - 1.\n",
        "    w2 = np.minimum(weights.max(), max_weight) - weights.max()\n",
        "    return np.abs(w1) + np.abs(w2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkBr7YIWomJc"
      },
      "source": [
        "# constraint tuple\n",
        "#cons = ({'type' : 'eq', 'fun' : check_sum})\n",
        "#cons = ({'type' : 'eq', 'fun' : check_sum}, {'type' : 'eq', 'fun' : check_max_weight}) # did not work\n",
        "cons = ({'type' : 'eq', 'fun' : check_weights}) # using this workaround instead"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rp2cX83onma"
      },
      "source": [
        "n_stocks = df_stocks.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qscX04Z8ooB6"
      },
      "source": [
        "bounds = tuple([(0,1) for i in range(n_stocks)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eJXj1B4opwi"
      },
      "source": [
        "init_guess = np.ones(n_stocks) / n_stocks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ep3QM3kow23"
      },
      "source": [
        "## Definir parâmetros de previsão"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8WGdb1zorgr"
      },
      "source": [
        "# the start date of the fist prediction (year, month, day)\n",
        "day_start = datetime.datetime(2019,1,1).date()\n",
        "\n",
        "# total number of months to run the prediction\n",
        "n_months_run = 28\n",
        "\n",
        "# training months before current prediction\n",
        "n_months_train = 12\n",
        "\n",
        "# portfolio weights (before re-balancing)\n",
        "max_weight = 0.25  # used in the constraint function\n",
        "min_weight = 0.02  # used in the running prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b6uPwH-o1qg"
      },
      "source": [
        "# Previsão mensal em execução"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ0ZL3zno53L"
      },
      "source": [
        "delta_month = dateutil.relativedelta.relativedelta(months=+1)\n",
        "delta_day = dateutil.relativedelta.relativedelta(days=+1)\n",
        "\n",
        "valid_start = day_start\n",
        "valid_end = valid_start + delta_month - delta_day\n",
        "\n",
        "train_start = valid_start - n_months_train*delta_month\n",
        "train_end = valid_start - delta_day\n",
        "\n",
        "time = []\n",
        "p = []\n",
        "b1 = []\n",
        "b2 = []\n",
        "b3 = []\n",
        "\n",
        "\n",
        "#\n",
        "for i in range(n_months_run):\n",
        "    \n",
        "    # dataframes\n",
        "    df_train = df_stocks.truncate(before=train_start, after=train_end)\n",
        "    df_valid = df_stocks.truncate(before=valid_start, after=valid_end)\n",
        "    df_valid_bench = df_bench.truncate(before=valid_start, after=valid_end)\n",
        "    \n",
        "    # calculating log returns of the training data\n",
        "    log_ret = np.log( df_train.divide(df_train.shift(1, axis=0), axis=0) ).iloc[2:]\n",
        "    # notice that log_ret is used by the function `get_ret_vol_sr` and, consequently,\n",
        "    # the `neg_sharpe` function    \n",
        "      \n",
        "    \n",
        "    # calculating optimized weights\n",
        "    opt_results = minimize(neg_sharpe, init_guess, method='SLSQP', bounds=bounds, constraints=cons)\n",
        "    \n",
        "    weights = opt_results.x\n",
        "    \n",
        "    \n",
        "    # Weight Re-balancing\n",
        "    idx = np.where(opt_results.x>=min_weight)[0]\n",
        "    weights = weights[idx]\n",
        "    weights /= weights.sum()\n",
        "    \n",
        "    labels = log_ret.columns[idx]\n",
        "    \n",
        "    # using the portfolio weights on the validation data\n",
        "    df1 = df_valid[labels]\n",
        "    df1 = df1/df1.iloc[0] # percentage return of the portfolio\n",
        "    df2 = (df1 * weights).sum(axis=1)\n",
        "    df2 = df2/df2.iloc[0] # percentage return of the portfolio\n",
        "    \n",
        "    # percentage return of the benchmarks\n",
        "    df2b = df_valid_bench/df_valid_bench.iloc[0]\n",
        "    \n",
        "    time.append(valid_start.strftime('%Y/%m'))\n",
        "    p.append(df2.iloc[-1])\n",
        "    b1.append(df2b['BVSP'].iloc[-1])\n",
        "    b2.append(df2b['USDBRL'].iloc[-1])\n",
        "    b3.append(df1.mean(axis=1).iloc[-1]) # Simple average of all stocks\n",
        "    \n",
        "    print('\\nStart: {}, Portfolio: {:.2f}, iBovespa: {:.2f}, Dolar: {:.2f}, Avg. : {:.2f}'.format(time[-1], p[-1],\n",
        "                                                                                                 b1[-1], b2[-1], b3[-1]))\n",
        "    \n",
        "    for l,w in zip(labels, weights):\n",
        "        print('  > {} : {:.2f}'.format(l, w))\n",
        "\n",
        "    \n",
        "    # time update for the next loop\n",
        "    valid_start += delta_month\n",
        "    valid_end  = valid_start + delta_month - delta_day\n",
        "    \n",
        "    train_start += delta_month\n",
        "    train_end = valid_start - delta_day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPvjA6IVpgPb"
      },
      "source": [
        "## Apresentando os resultados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wT2hJkJRqQNx"
      },
      "source": [
        "d = {'Date' : pd.to_datetime(time),\n",
        "    'Portfolio' : p,\n",
        "    'iBovespa' : b1,\n",
        "    'Dolar' : b2,\n",
        "    'Avg. BVSP' : b3}\n",
        "df_results = pd.DataFrame(data=d)\n",
        "df_results.set_index('Date', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDBvahyGqQ4Y"
      },
      "source": [
        "print('Average - Monthly returns:')\n",
        "df_results.mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKnAdxLdqS8Q"
      },
      "source": [
        "print('std - Monthly returns:')\n",
        "df_results.std(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L35WvQXyqYCI"
      },
      "source": [
        "ax = df_results.plot(style='-o')\n",
        "ax.axhline(y=1.0, color='gray', linestyle='--', lw=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}